cwd: ${hydra:runtime.cwd}
project: livecell_noise2same
device: 0
seed: 56
check: false
evaluate:
  key: image
denoiser:
  _target_: noise2same.denoiser.Denoiser
training:
  steps_per_epoch: 1000
  steps: 20000
  batch_size: 64
  num_workers: 4
  crop: 64
  validate: true
  val_partition: 1.0
  val_batch_size: 4
  monitor: val_mse
  amp: true
dataset:
  n_dim: 2
  n_channels: 1
  standardize: true
  pad_divisor: ${backbone.pad_divisor}
  _target_: noise2same.dataset.LiveCellDataset
  path:  /gpfs/space/projects/transformers_uss
  add_blur_and_noise: false
  mean: 29.1322
  std: 24.1591
optimizer:
  _target_: lion_pytorch.Lion
  lr: 1.0e-06
  weight_decay: 1.0e-08
experiment: fmd
dataset_train:
  mode: train
  transforms:
    _target_: noise2same.dataset.util.training_augmentations_2d
    crop: 64
  input_size: ${.transforms.crop}
dataset_valid:
  mode: val
  transforms:
    _target_: noise2same.dataset.util.validation_transforms_2d
    crop: 64
dataset_test:
  mode: test_random_50

scheduler:
  _target_: noise2same.scheduler.ExponentialDecayScheduler
  decay_rate: 0.5
  decay_steps: 5e3  # how many steps to decrease by decay rate
  staircase: True  # integer division

backbone_name: unet

backbone:
  _target_: noise2same.backbone.unet.UNet
  in_channels: ${dataset.n_channels}
  n_dim: ${dataset.n_dim}
  base_channels: 96
  kernel_size: 3
  depth: 3
  encoding_block_sizes:
  - 1
  - 1
  - 0
  decoding_block_sizes:
  - 1
  - 1
  downsampling:
  - conv
  - conv
  downsampling_factor:
  - 2
  - 2
  skip_method: concat
  pad_divisor: ${eval:2 ** ${.depth}}  # used in dataset config

#head:
#  _target_: noise2same.backbone.unet.RegressionHead
#  in_channels: ${backbone.base_channels}
#  out_channels: ${dataset.n_channels}
#  n_dim: ${dataset.n_dim}
head:
  _target_: torch.nn.Identity


